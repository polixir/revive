{
    "base_config": [
        {
            "name": "global_seed",
            "abbreviation": "gs",
            "description": "Set the random number seed for the experiment.",
            "type": "int",
            "default": 42
        },
        {
            "name": "val_split_ratio",
            "abbreviation": "vsr",
            "description": "Ratio to split validate dataset if it is not explicitly given.",
            "type": "float",
            "default": 0.5
        },
        {
            "name": "val_split_mode",
            "abbreviation": "vsm",
            "description": "Mode of auto splitting training and validation dataset, choose from `outside_traj` and `inside_traj`. `outside_traj` means the split is happened outside the trajectories, one trajectory can only be in one dataset. `inside_traj` means the split is happened inside the trajectories, former part of one trajectory is in training set, later part is in validation set.",
            "type": "str",
            "default": "outside_traj"
        },
        {
            "name": "ignore_check",
            "abbreviation": "igc",
            "description": "Flag to ignore data related check, force training.",
            "type": "bool",
            "default": false
        },
        {
            "name": "venv_rollout_horizon",
            "abbreviation": "vrh",
            "description": "Length of sampled trajectory, validate only if the algorithm works on sequential data.",
            "type": "int",
            "default": 100
        },
        {
            "name": "transition_mode",
            "abbreviation": "tsm",
            "description": "Mode of transition, choose from `global` and `local`.",
            "type": "str",
            "default": "global"
        },
        {
            "name": "venv_gpus_per_worker",
            "abbreviation": "vgpw",
            "description": "Number of gpus per worker in venv training, small than 1 means launch multiple workers on the same gpu.",
            "type": "float",
            "default": 1.0
        },
        {
            "name": "venv_metric",
            "description": "Metric used to evaluate the trained venv, choose from `nll`, `mae`, `mse`, `wdist`.",
            "type": "str",
            "default": "mae"
        },
        {
            "name": "venv_algo",
            "description": "Algorithm used in venv training. There are currently three algorithms to choose from, `bc` and `revive_p`.",
            "type": "str",
            "default": "revive_p"
        },
        {
            "name": "policy_gpus_per_worker",
            "abbreviation": "pgpw",
            "description": "Number of gpus per worker in venv training, small than 1 means launch multiple workers on the same gpu.",
            "type": "float",
            "default": 1.0
        },
        {
            "name": "num_venv_in_use",
            "abbreviation": "nviu",
            "description": "Max number of venvs used in policy training, clipped when there is no enough venvs available.",
            "type": "float",
            "default": 1
        },
        {
            "name": "behavioral_policy_init",
            "abbreviation": "bpi",
            "description": "Whether to use the learned behavioral policy to as the initialization policy training.",
            "type": "bool",
            "default": true
        },
        {
            "name": "test_horizon",
            "abbreviation": "th",
            "description": "Rollout length of the venv test.",
            "type": "int",
            "default": 100
        },
        {
            "name": "action_steps",
            "description": "Use the action for some steps in env.",
            "type": "int",
            "default": 1
        },
        {
            "name": "train_venv_trials",
            "abbreviation": "tvt",
            "description": "Number of total trails searched by the search algorithm in venv training.",
            "type": "int",
            "default": 25
        },
        {
            "name": "train_policy_trials",
            "abbreviation": "tpt",
            "description": "Number of total trails searched by the search algorithm in policy training.",
            "type": "int",
            "default": 10
        }
    ],
    "venv_algo_config": {
        "revive_p": [
            {
                "name": "revive_batch_size",
                "description": "Batch size of training process.",
                "abbreviation": "mbs",
                "type": "int",
                "default": 1024
            },
            {
                "name": "revive_epoch",
                "description": "Number of epcoh for the training process",
                "abbreviation": "mep",
                "type": "int",
                "default": 5000
            },
            {
                "name": "fintune",
                "abbreviation": "bet",
                "type": "int",
                "default": 1
            },
            {
                "name": "policy_hidden_features",
                "description": "Number of neurons per layer of the policy network.",
                "abbreviation": "phf",
                "type": "int",
                "default": 256
            },
            {
                "name": "policy_hidden_layers",
                "description": "Depth of policy network.",
                "abbreviation": "phl",
                "type": "int",
                "default": 4
            },
            {
                "name": "policy_backbone",
                "description": "Backbone of policy network.",
                "abbreviation": "pb",
                "type": "str",
                "default": "res"
            },
            {
                "name": "transition_hidden_features",
                "description": "Number of neurons per layer of the transition network.",
                "abbreviation": "thf",
                "type": "int",
                "default": 256
            },
            {
                "name": "transition_hidden_layers",
                "abbreviation": "thl",
                "type": "int",
                "default": 4
            },
            {
                "name": "transition_backbone",
                "description": "Backbone of Transition network.",
                "abbreviation": "tb",
                "type": "str",
                "default": "res"
            },
            {
                "name": "matcher_hidden_features",
                "description": "Number of neurons per layer of the matcher network.",
                "abbreviation": "dhf",
                "type": "int",
                "default": 256
            },
            {
                "name": "matcher_hidden_layers",
                "description": "Depth of the matcher network.",
                "abbreviation": "dhl",
                "type": "int",
                "default": 4
            },
            {
                "name": "g_steps",
                "description": "The number of update rounds of the generator in each epoch.",
                "type": "int",
                "default": 1,
                "search_mode": "grid",
                "search_values": "[1,3,5]"
            },
            {
                "name": "d_steps",
                "description": "Number of update rounds of matcher in each epoch.",
                "type": "int",
                "default": 1,
                "search_mode": "grid",
                "search_values": "[1,3,5]"
            },
            {
                "name": "g_lr",
                "description": "Initial learning rate of the generator.",
                "type": "float",
                "default": 4e-05,
                "search_mode": "continuous",
                "search_values": "[1e-06,0.0001]"
            },
            {
                "name": "d_lr",
                "description": "Initial learning rate of the matcher.",
                "type": "float",
                "default": 0.0006,
                "search_mode": "continuous",
                "search_values": "[1e-06,0.001]"
            },
            {
                "name": "mae_reward_weight",
                "description": "reward = (1-mae_reward_weight)*matcher_reward + mae_reward_weight*mae_reward.",
                "type": "float",
                "default": 0.25
            },
            {
                "name": "generator_data_repeat",
                "description": "Repeat rollout more data to train generator.",
                "type": "int",
                "default": 1
            }
        ],
        "bc": [
            {
                "name": "bc_batch_size",
                "description": "Batch size of training process.",
                "abbreviation": "bbs",
                "type": "int",
                "default": 256
            },
            {
                "name": "bc_epoch",
                "description": "Number of epcoh for the training process",
                "abbreviation": "bep",
                "type": "int",
                "default": 500
            },
            {
                "name": "policy_hidden_features",
                "description": "Number of neurons per layer of the policy network.",
                "abbreviation": "phf",
                "type": "int",
                "default": 256
            },
            {
                "name": "policy_hidden_layers",
                "description": "Depth of policy network.",
                "abbreviation": "phl",
                "type": "int",
                "default": 4,
                "search_mode": "grid",
                "search_values": "[3,4,5]"
            },
            {
                "name": "policy_backbone",
                "description": "Backbone of policy network.",
                "abbreviation": "pb",
                "type": "str",
                "default": "res",
                "search_mode": "grid",
                "search_values": "['mlp','res']"
            },
            {
                "name": "g_lr",
                "description": "Initial learning rate of the training process.",
                "type": "float",
                "default": 0.0001,
                "search_mode": "continuous",
                "search_values": "[1e-06,0.001]"
            },
            {
                "name": "loss_type",
                "description": "Bc support different loss function(\"log_prob\", \"mae\", \"mse\").",
                "type": "str",
                "default": "log_prob"
            }
        ]
    },
    "policy_algo_config": {
        "ppo": [
            {
                "name": "ppo_batch_size",
                "description": "Batch size of training process.",
                "abbreviation": "pbs",
                "type": "int",
                "default": 256
            },
            {
                "name": "ppo_epoch",
                "description": "Number of epcoh for the training process",
                "abbreviation": "bep",
                "type": "int",
                "default": 200
            },
            {
                "name": "ppo_rollout_horizon",
                "description": "Rollout length of the policy train.",
                "abbreviation": "prh",
                "type": "int",
                "default": 100
            },
            {
                "name": "policy_hidden_features",
                "description": "Number of neurons per layer of the policy network.",
                "abbreviation": "phf",
                "type": "int",
                "default": 256
            },
            {
                "name": "policy_hidden_layers",
                "description": "Depth of policy network.",
                "abbreviation": "phl",
                "type": "int",
                "default": 4
            },
            {
                "name": "policy_backbone",
                "description": "Backbone of policy network.",
                "abbreviation": "pb",
                "type": "str",
                "default": "mlp"
            },
            {
                "name": "g_lr",
                "description": "Initial learning rate of the training process.",
                "type": "float",
                "default": 4e-05,
                "search_mode": "continuous",
                "search_values": "[1e-06,0.001]"
            }
        ]
    }
}